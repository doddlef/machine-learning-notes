  
## Probabilistic learner  
* goal is classification, build up a supervise model  
* for each class c:  
  * find all examples of c in the training data  
  * count the number of times T has been observed  
* given an instance `T`, which class `c` is most likely  
* so c is which max $P(c | T)$  
  
## Naive Bayes solution  
* assume different attributes are statistically independent, conditional on class  
* Compute `P(c | T)` from `P(T | C)` using bayes rule  
  * we can compute `p(H | x)` when `P(x |H)` cna be estimated  
* to max $P(c | T)$, we need to max ${P(T | c) P(c)}$  
* suppose T may be composed by many props <$x_{1}, x_{2}, ..., x_{n}$>  
  * assume all attributes are independent, conditional  
  * = P(x1 | c) * ... * P(xn | c)  
* the P(c) can be estimated from the frequency in training set  
  * Good if training set correctly reflects the distribution  
  * but cannot apply to a new situation  
  
## Zero values and smoothing  
* if any P($x_{i}$ | $c_{j}$) = 0, the final value will be zero  
  * we need a lot of data  
  * 0s indicates it's probably rare  
  
### Solution 1  
* treat unobserved events as possible but unlikely (all probabilities > 0)  
  * replace 0 with a small positive constant $\varepsilon$  
    * much less than (1/N) (number of instances)  
### Solution 2  
* increase all counts by 1 (_Laplace smoothing_)  
  * or more general: increase by a, which between 0 and 1  
      * Smoothed: $P_{i} = \frac{x_{i} + a}{N + ad}$  
        * adding a `ad` to ensure the sum probability is `1`  
    * probabilities are changed drastically when few instances, but smaller with large  
  * overestimate the likelihood of rare events  
### More solutions  
* add-k: laplace smoothing but k > 1  
* good-turing estimation: uses the observed counts of different events to estimate the probability of this event  
* regression  
  
## Missing values  
if an instances is missing some attribute  
* in test can be ignored: compute the likelihood of each class from non-missing values  
* in training can also be ignored: not include in counts and probabilities  
  
## summary  
* not need perfect estimate for every class, just which is most likely  
* ignoring the fact that some attributes are correlated tends to make all the class probabilities higher, but no typically change rank  
* robust to small errors in estimating P(x | c)  
* simple to build fast  
* scale well to high dimensional  
* explainable  
* inaccurate when missing many P(x | c)  
* problematic for complex systems (like images?)